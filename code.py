# -*- coding: utf-8 -*-
"""Code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Rqx4TDT4kgIxTxWu2r7n5y1_gOJYTcIS
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install -q ultralytics transformers accelerate gradio

# Step 2: Save the chatbot code
code = r'''
#!/usr/bin/env python3
# yolo_blip2_chatbot.py

from _future_ import annotations
import os, sys
from typing import List, Tuple
import torch
from PIL import Image
import gradio as gr
from ultralytics import YOLO
from transformers import Blip2Processor, Blip2ForConditionalGeneration

# ---------------------------
# Config
# ---------------------------
# Use a smaller BLIP2 model for Colab (fits into 15GB VRAM)
MODEL_NAME = os.environ.get("BLIP2_MODEL", "Salesforce/blip2-opt-2.7b")

YOLO_WEIGHTS = os.environ.get("YOLO_WEIGHTS", "yolov8s.pt")
MAX_OBJECTS_IN_PROMPT = int(os.environ.get("MAX_OBJECTS_IN_PROMPT", 12))
MIN_CONFIDENCE = float(os.environ.get("MIN_CONFIDENCE", 0.25))
GEN_MAX_NEW_TOKENS = int(os.environ.get("GEN_MAX_NEW_TOKENS", 128))
GEN_TEMPERATURE = float(os.environ.get("GEN_TEMPERATURE", 0.3))
USE_FP16 = True

# ---------------------------
# Device helper
# ---------------------------
def get_device_and_dtype() -> Tuple[str, torch.dtype]:
    if torch.cuda.is_available():
        return "cuda", (torch.float16 if USE_FP16 else torch.float32)
    return "cpu", torch.float32

# ---------------------------
# Model loading
# ---------------------------
def load_models() -> Tuple[YOLO, Blip2ForConditionalGeneration, Blip2Processor]:
    device, dtype = get_device_and_dtype()
    yolo = YOLO(YOLO_WEIGHTS)
    blip2 = Blip2ForConditionalGeneration.from_pretrained(
        MODEL_NAME,
        torch_dtype=dtype,
        device_map="auto"
    )
    processor = Blip2Processor.from_pretrained(MODEL_NAME)
    print(f"[INFO] Loaded BLIP-2: {MODEL_NAME} on {device} ({dtype})")
    return yolo, blip2, processor

# ---------------------------
# Detection helper
# ---------------------------
def detect_objects(yolo: YOLO, image: Image.Image, min_conf: float = MIN_CONFIDENCE):
    results = yolo(image)
    if len(results) == 0: return [], image
    r = results[0]
    boxes = getattr(r, "boxes", None)
    if boxes is None or boxes.xyxy is None or len(boxes) == 0:
        return [], image

    cls_ids = boxes.cls.detach().cpu().numpy().astype(int).tolist()
    confs = boxes.conf.detach().cpu().numpy().tolist()
    names_map = r.names if hasattr(r, "names") else getattr(yolo.model, "names", {})
    items = []
    for cid, conf in zip(cls_ids, confs):
        if conf >= min_conf:
            name = names_map.get(int(cid), f"class_{int(cid)}")
            items.append(name)

    annotated = r.plot()
    annotated = annotated[..., ::-1]
    annotated_pil = Image.fromarray(annotated)
    return items, annotated_pil

# ---------------------------
# Prompt builder
# ---------------------------
def build_enriched_prompt(user_question: str, detected_objects_str: str) -> str:
    user_question = (user_question or "").strip()
    prefix = "You are a helpful vision assistant. Answer concisely and accurately.\n"
    if detected_objects_str:
        context = f"Detected items: {detected_objects_str}.\n"
    else:
        context = "No reliable objects were detected.\n"
    instruction = (
        "Only answer using information that can be inferred from the image. "
        "For numerical questions, always provide an exact number. "
        "For visual features, always state the color if visible. "
        "If you are not certain, state that you are not certain. Now answer:\n"
    )
    if user_question:
        qline = f"Question: {user_question}\nAnswer:"
    else:
        qline = "Question: Describe the image.\nAnswer:"
    return prefix + context + instruction + qline

# ---------------------------
# BLIP-2 answer
# ---------------------------
@torch.inference_mode()
def blip2_answer(blip2, processor, image: Image.Image, prompt: str, device=None):
    device = device or ("cuda" if torch.cuda.is_available() else "cpu")
    inputs = processor(images=image, text=prompt, return_tensors="pt").to(device)
    out = blip2.generate(**inputs, max_new_tokens=GEN_MAX_NEW_TOKENS, temperature=GEN_TEMPERATURE)
    text = processor.decode(out[0], skip_special_tokens=True)
    return text.strip()

# ---------------------------
# App (FINAL CORRECTED VERSION)
# ---------------------------
class App:
    def _init_(self):
        self.yolo, self.blip2, self.processor = load_models()
        self.device, _ = get_device_and_dtype()

    @torch.inference_mode()
    def infer(self, image: Image.Image, question: str, history: List[Tuple[str, str]]):
        if image is None:
            return "Please upload an image.", None, history

        # 1. Detect objects and get the raw list
        objects, annotated = detect_objects(self.yolo, image)

        # 2. Count the occurrences of each object type in code
        object_counts = {}
        for obj in objects:
            object_counts[obj] = object_counts.get(obj, 0) + 1

        # 3. Create a descriptive string with the exact counts
        count_str = ", ".join([f"{count} {obj}" for obj, count in object_counts.items()])

        # 4. Build the prompt with the correct structure
        history_str = "\nPrevious Conversation:\n"
        for h_q, h_a in history:
            history_str += f"User: {h_q}\nBot: {h_a}\n"

        # The prompt now separates the context from the question
        final_prompt = build_enriched_prompt(f"{history_str}\nUser: {question}", count_str)

        ans = blip2_answer(self.blip2, self.processor, image, final_prompt, self.device)

        history.append((question, ans))

        return ans, annotated, history

def build_interface(app: App) -> gr.Blocks:
    with gr.Blocks() as demo:
        gr.Markdown("# Conversational Image Recognition (YOLO + BLIP-2)")

        # Initialize an invisible state variable to store conversation history
        history = gr.State([])

        with gr.Row():
            with gr.Column(scale=1):
                img_in = gr.Image(type="pil", label="Upload Image")
                q_in = gr.Textbox(label="Your Question", placeholder="What is the person holding?")
                run_btn = gr.Button("Answer")
            with gr.Column(scale=1):
                ans_out = gr.Textbox(label="Answer", lines=6)
                img_out = gr.Image(type="pil", label="Detected Objects (Annotated)")

        # Pass the history as an input and an output to the infer function
        run_btn.click(app.infer, inputs=[img_in, q_in, history], outputs=[ans_out, img_out, history])

    return demo

if _name_ == "_main_":
    app = App()
    ui = build_interface(app)
    ui.launch(share=True)
'''
with open("yolo_blip2_chatbot.py", "w") as f:
    f.write(code)

with open("/content/drive/MyDrive/yolo_blip2_chatbot_improved.py", "w") as f:
    f.write(code_with_smaller_model)

# Step 4: Run the Python script
!python /content/drive/MyDrive/yolo_blip2_chatbot_improved.py

with open("/content/drive/MyDrive/yolo_blip2_chatbot_improved.py", "w") as f:
    f.write(code_with_smaller_model)

# Step 4: Run the Python script
!python /content/drive/MyDrive/yolo_blip2_chatbot_improved.py

